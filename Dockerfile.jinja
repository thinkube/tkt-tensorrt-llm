FROM nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2

WORKDIR /app

# Copy application code
COPY server.py .
COPY thinkube_theme.py .
COPY entrypoint.sh .

# Copy Thinkube icons (pre-generated)
RUN mkdir -p /app/icons
COPY tk_ai.svg tk_ai.png /app/icons/

# Copy and install requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Model configuration from template
ENV MODEL_ID="{{ model_id }}"
ENV HF_TOKEN="${HF_TOKEN}"

# Make entrypoint executable
RUN chmod +x entrypoint.sh

# Expose ports: 7860 for Gradio, 8355 for TensorRT-LLM API
EXPOSE 7860 8355

# Run the entrypoint script
CMD ["./entrypoint.sh"]
