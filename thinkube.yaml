apiVersion: thinkube.io/v1
kind: ThinkubeDeployment
metadata:
  name: "{{ project_name }}"

spec:
  containers:
    - name: inference
      build: .
      port: 7860
      size: xlarge
      gpu:
        count: 1
        memory: "80Gi"  # Auto-gets: hostIPC=true + /mlflow-models mount
      health: /health
      test:
        enabled: false  # Testing ML models requires GPU, skip in CI/CD
      env:
        - name: MLFLOW_KEYCLOAK_TOKEN_URL
          valueFrom:
            secretKeyRef:
              name: mlflow-auth-config
              key: keycloak-token-url
        - name: MLFLOW_KEYCLOAK_CLIENT_ID
          valueFrom:
            secretKeyRef:
              name: mlflow-auth-config
              key: client-id
        - name: MLFLOW_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              name: mlflow-auth-config
              key: client-secret
        - name: MLFLOW_AUTH_USERNAME
          valueFrom:
            secretKeyRef:
              name: mlflow-auth-config
              key: username
        - name: MLFLOW_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mlflow-auth-config
              key: password

  routes:
    - path: /
      to: inference